# Evaluation using Claude 3.5 Sonnet and GPT-4o

Follow the following steps to evaluate the responses generated by the VLMs to the questions in our dataset. 

## Requirements

1. Create/Log into your [OpenAI console](https://platform.openai.com/settings/organization/billing/overview) and pay to use their API

**Warning: Each model will cost you around $20 to do the evaluation**

2. Create/Log into your [Anthropic console](https://console.anthropic.com/settings/billing) and pay to use their API

**Warning: Each model will cost you around $20 to do the evaluation**

## Claude Evaluation

1. Create a directory named ``Claude`` and go to it
   
``cd Claude``

2. Download the file ``captions_val2017.json`` and move it to ``Claude``

### Evaluation for Experiment A

1. Download the ``claude_eval_expA.py`` file and move it to the ``Claude`` directory

2. Open ``claude_eval_expA.py`` and modify the code under 3 TODOs in this script

- Line 18: Replace with actual path to your test/images directory (downloaded from our dataset)
- Line 27: Determine the VLM you are evaluating here (e.g. cogvlm2, llavanext, finetuned_qwenvlchat, etc.)
- Line 53: Replace with actual path to your VLM's response CSV file

3. Save the code, go to ``Claude`` and run the code there

``python claude_eval_expA.py``

This will then create a CSV file that contains the image file paths, the types of questions (Association, Intervention, and Counterfactual), and the ratings. This can then be used to calculate the average score for each question type.

### Evaluation for Experiment B

1. Download the ``claude_eval_expB.py`` file and move it to the ``Claude`` directory

2. Open ``claude_eval_expB.py`` and modify the code under 3 TODOs in this script

- Line 18: Replace with actual path to your test/images directory (downloaded from our dataset)
- Line 27: Determine the VLM you are evaluating here (e.g. cogvlm2, llavanext, finetuned_qwenvlchat, etc.)
- Line 53: Replace with actual path to your VLM's response CSV file

3. Save the code, go to ``Claude`` and run the code there

``python claude_eval_expB.py``

This will then create a CSV file that contains the image file paths, the types of questions (Association, Intervention, and Counterfactual), the graph ratings, and the text response ratings. This can then be used to calculate the average score for each question type.

## GPT-4o Evaluation

1. Create a directory named ``GPT-4`` and go to it
   
``cd GPT-4``

2. Download the file ``captions_val2017.json`` and move it to ``GPT-4``

### Evaluation for Experiment A

1. Download the ``gpt4o_eval_expA.py`` file and move it to the ``GPT-4`` directory

2. Open ``gpt4o_eval_expA.py`` and modify the code under 4 TODOs in this script

- Line 2: Replace with your OpenAI API key
- Line 13: Replace with actual path to your test/images directory (downloaded from our dataset)
- Line 22: Determine the VLM you are evaluating here (e.g. cogvlm2, llavanext, finetuned_qwenvlchat, etc.)
- Line 58: Replace with actual path to your VLM's response CSV file

3. Save the code, go to ``GPT-4`` and run the code there

``python gpt4o_eval_expA.py``

This will then create a CSV file that contains the image file paths, the types of questions (Association, Intervention, and Counterfactual), and the ratings. This can then be used to calculate the average score for each question type.

### Evaluation for Experiment B

1. Download the ``gpt4o_eval_expB.py`` file and move it to the ``GPT-4`` directory

2. Open ``gpt4o_eval_expB.py`` and modify the code under 4 TODOs in this script

- Line 2: Replace with your OpenAI API key
- Line 13: Replace with actual path to your test/images directory (downloaded from our dataset)
- Line 22: Determine the VLM you are evaluating here (e.g. cogvlm2, llavanext, finetuned_qwenvlchat, etc.)
- Line 58: Replace with actual path to your VLM's response CSV file

3. Save the code, go to ``GPT-4`` and run the code there

``python gpt4o_eval_expB.py``

This will then create a CSV file that contains the image file paths, the types of questions (Association, Intervention, and Counterfactual), the graph ratings, and the text response ratings. This can then be used to calculate the average score for each question type.

## Calculate the Average Score for Each Question Type

1. Download the file ``calculate_avg_scores.py``

2. Open ``calculate_avg_scores.py`` and modify the code under 3 TODOs in this script

- Line 4: Determine whether you are evaluating Experiment A or Experiment B
- Line 9: Replace with your model's evaluation CSV file (by Claude or GPT-4o) for Experiment A
- Line 21: Replace with your model's evaluation CSV file (by Claude or GPT-4o) for Experiment B

This will print out the average score for each question type in the console command.
